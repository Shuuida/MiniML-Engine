# Cuantificaci√≥n en MiniML: Gu√≠a Completa

## Tabla de Contenidos
1. [Introducci√≥n](#introducci√≥n)
2. [Arquitectura de Cuantificaci√≥n](#arquitectura-de-cuantificaci√≥n)
3. [M√©todos de Cuantificaci√≥n](#m√©todos-de-cuantificaci√≥n)
4. [Proceso de Cuantificaci√≥n](#proceso-de-cuantificaci√≥n)
5. [Exportaci√≥n a Firmware C](#exportaci√≥n-a-firmware-c)
6. [Tabla Comparativa](#tabla-comparativa)
7. [Limitaciones Actuales](#limitaciones-actuales)
8. [Recomendaciones para Proyectos Embebidos](#recomendaciones-para-proyectos-embebidos)
9. [Ejemplos de Uso](#ejemplos-de-uso)

---

## Introducci√≥n

MiniML implementa **Post-Training Quantization (PTQ)** para redes neuronales, permitiendo reducir el tama√±o del modelo y acelerar la inferencia en microcontroladores de bajo costo mediante el uso de aritm√©tica de enteros de 8 bits (int8) en lugar de punto flotante de 32 bits.

### Beneficios Clave
- **Reducci√≥n de memoria**: ~75% menos espacio (int8 vs float32)
- **Aceleraci√≥n**: Operaciones enteras m√°s r√°pidas en MCUs sin FPU
- **Compatibilidad CMSIS-NN**: Integraci√≥n con kernels optimizados de ARM
- **Precisi√≥n preservada**: P√©rdida t√≠pica < 2% en accuracy

---

## Arquitectura de Cuantificaci√≥n

### Componentes Principales

#### 1. **MiniNeuralNetwork** (`ml_runtime.py`)
Clase base que implementa:
- Calibraci√≥n de activaciones (`calibrate()`)
- Cuantificaci√≥n de pesos y biases (`quantize()`)
- Exportaci√≥n nativa a C (`to_arduino_code()`)

#### 2. **CMSISAdapter** (`adapters/cmsis_nn/adapter.py`)
Adaptador avanzado que genera c√≥digo compatible con:
- **CMSIS-NN**: Kernels optimizados de ARM para Cortex-M
- **Fallback port√°til**: Implementaci√≥n en C est√°ndar sin dependencias

### Flujo de Cuantificaci√≥n

```
Entrenamiento (Float32)
    ‚Üì
Calibraci√≥n (calibrate())
    ‚Üì [Calcula act_scales: input, hidden, output]
Cuantificaci√≥n (quantize())
    ‚Üì [Convierte pesos a int8, biases a int32]
Exportaci√≥n (export_to_c())
    ‚Üì [Genera c√≥digo C optimizado]
Firmware C (int8 inference)
```

---

## M√©todos de Cuantificaci√≥n

### 1. Cuantificaci√≥n Sim√©trica por Capa (Per-Layer Symmetric)

**Implementaci√≥n**: M√©todo por defecto en `MiniNeuralNetwork.quantize()`

#### Caracter√≠sticas:
- **Pesos**: Cuantificados a `int8` con rango `[-127, 127]`
- **Zero-point**: Impl√≠citamente 0 (symmetric quantization)
- **Escala por capa**: Una escala por matriz de pesos
- **Biases**: Cuantificados a `int32` para preservar precisi√≥n

#### F√≥rmulas:

**Para Pesos (W):**
```
abs_max = max(|min(W)|, |max(W)|)
scale_w = abs_max / 127.0
q_w = round(w / scale_w)  # Clipped to [-127, 127]
```

**Para Biases (B):**
```
effective_scale = input_scale * scale_w
b_int32 = round(b / effective_scale)  # Clipped to int32 range
```

**Multiplicador de Requantizaci√≥n:**
```
requant_mult = effective_scale / output_scale
```

#### Ventajas:
- ‚úÖ Implementaci√≥n simple y r√°pida
- ‚úÖ Bajo overhead computacional
- ‚úÖ Compatible con CMSIS-NN
- ‚úÖ Buena precisi√≥n para redes peque√±as-medianas

#### Desventajas:
- ‚ùå Menor precisi√≥n que per-channel para redes grandes
- ‚ùå Sensible a outliers en distribuci√≥n de pesos

---

### 2. Cuantificaci√≥n con CMSIS-NN (Fixed-Point)

**Implementaci√≥n**: `CMSISAdapter.generate_c()`

#### Caracter√≠sticas:
- **Pesos**: `int8_t` almacenados en arrays alineados
- **Biases**: `int32_t` pre-cuantificados
- **Multiplicadores**: Convertidos a formato Q31 (significand + shift)
- **Activaciones**: `int8_t` en inferencia completa

#### Formato Q31 para Multiplicadores:
```python
def _quantize_multiplier(real_multiplier: float) -> Tuple[int, int]:
    significand, shift = math.frexp(real_multiplier)
    q_mult = int(round(significand * (1 << 31)))
    # Ajuste para evitar overflow
    if q_mult == (1 << 31):
        q_mult //= 2
        shift += 1
    return q_mult, shift
```

#### Ventajas:
- ‚úÖ M√°xima velocidad en Cortex-M (kernels SIMD)
- ‚úÖ Inferencia completamente en int8 (sin float)
- ‚úÖ Bajo consumo de energ√≠a
- ‚úÖ Compatible con ARM CMSIS-NN

#### Desventajas:
- ‚ùå Requiere librer√≠a CMSIS-NN (no port√°til)
- ‚ùå Limitado a arquitecturas ARM Cortex-M
- ‚ùå Mayor complejidad de implementaci√≥n

---

### 3. Modo H√≠brido AVR (Arduino 8-bit)

**Implementaci√≥n**: `MiniNeuralNetwork.to_arduino_code()`

#### Caracter√≠sticas:
- **Pesos**: `int8_t` almacenados en `PROGMEM` (Flash)
- **Escalas**: `float` almacenadas en `PROGMEM`
- **C√°lculo**: H√≠brido (int8 storage, float compute)
- **Biases**: `float` originales en `PROGMEM`

#### Estrategia:
```c
// Lectura de peso cuantificado desde Flash
int8_t w = pgm_read_byte(&W1[i][j]);
// Descuantificaci√≥n on-the-fly
float dequantized = (float)w * scale * input;
```

#### Ventajas:
- ‚úÖ Ahorra SRAM (pesos en Flash, no RAM)
- ‚úÖ Ideal para AVR 8-bit (Arduino Uno/Nano)
- ‚úÖ No requiere FPU (usa float solo para escalas)
- ‚úÖ Port√°til (sin dependencias externas)

#### Desventajas:
- ‚ùå M√°s lento que int8 puro (conversiones float)
- ‚ùå Requiere FPU o emulaci√≥n de float
- ‚ùå Mayor uso de Flash que int8 puro

---

## Proceso de Cuantificaci√≥n

### Paso 1: Calibraci√≥n (`calibrate()`)

**Prop√≥sito**: Determinar los rangos de activaci√≥n para cada capa.

```python
def calibrate(self, dataset: List[List[float]]):
    """
    Calcula rangos de activaci√≥n (min/max) para Input, Hidden y Output.
    Esencial para cuantificaci√≥n int8 (Post-Training Quantization).
    """
    # Itera sobre el dataset y encuentra:
    # - max_in: m√°ximo absoluto de inputs
    # - max_hidden: m√°ximo absoluto de activaciones ocultas
    # - max_out: m√°ximo absoluto de outputs
    
    self.act_scales = {
        'input': max_in / 127.0,
        'hidden': max_hidden / 127.0,
        'output': max_out / 127.0
    }
```

**Notas Importantes**:
- Se ejecuta autom√°ticamente despu√©s de `fit()`
- Requiere dataset de calibraci√≥n (t√≠picamente el de entrenamiento)
- Los escalas se guardan en `act_scales` para uso posterior

### Paso 2: Cuantificaci√≥n (`quantize()`)

**Prop√≥sito**: Convertir pesos y biases de float32 a int8/int32.

```python
def quantize(self, per_channel: bool = True):
    """
    Cuantifica pesos a int8 y biases a int32.
    Requiere act_scales calculados previamente.
    """
    # Para cada capa:
    # 1. Calcula escala por fila de pesos
    # 2. Cuantifica pesos a int8
    # 3. Cuantifica biases a int32
    # 4. Calcula multiplicadores de requantizaci√≥n
    
    self.q_W1, self.i32_B1, self.requant_mult1, self.s_W1_list = ...
    self.q_W2, self.i32_B2, self.requant_mult2, self.s_W2_list = ...
    self.quantized = True
```

**Atributos Generados**:
- `q_W1`, `q_W2`: Matrices de pesos cuantificadas (int8)
- `i32_B1`, `i32_B2`: Vectores de bias cuantificados (int32)
- `requant_mult1`, `requant_mult2`: Multiplicadores de requantizaci√≥n (float)
- `s_W1_list`, `s_W2_list`: Escalas por fila de pesos (float)

### Paso 3: Exportaci√≥n (`export_to_c()`)

**Prop√≥sito**: Generar c√≥digo C optimizado para el firmware.

**Detecci√≥n Autom√°tica**:
- Si el modelo tiene `q_W1` ‚Üí Usa `CMSISAdapter` (preferido)
- Si falla ‚Üí Usa `to_arduino_code()` (fallback nativo)
- Si tiene scaler ‚Üí Incluye c√≥digo de preprocesamiento

---

## Exportaci√≥n a Firmware C

### Opci√≥n 1: CMSIS-NN (Recomendado para ARM Cortex-M)

**Generado por**: `CMSISAdapter.generate_c()`

**Caracter√≠sticas**:
- C√≥digo optimizado para `arm_fully_connected_s8()`
- Inferencia completamente en int8
- Fallback port√°til si CMSIS-NN no est√° disponible

**Estructura del C√≥digo**:
```c
// Arrays de datos (alineados para SIMD)
const int8_t W1[N] ALIGNED(4) = {...};
const int32_t B1[M] ALIGNED(4) = {...};
const int32_t MULT1[M] ALIGNED(4) = {...};
const int32_t SHIFT1[M] ALIGNED(4) = {...};

#ifdef CMSISNN_ENABLED
    // Usa kernels optimizados de ARM
    arm_fully_connected_s8(...);
#else
    // Fallback port√°til en C est√°ndar
    // Implementaci√≥n manual con loops
#endif
```

**Ventajas**:
- M√°xima velocidad en Cortex-M4/M7
- Bajo consumo de energ√≠a
- Inferencia pura int8 (sin float)

### Opci√≥n 2: Modo H√≠brido AVR (Arduino 8-bit)

**Generado por**: `MiniNeuralNetwork.to_arduino_code()`

**Caracter√≠sticas**:
- Pesos en `PROGMEM` (Flash)
- C√°lculo h√≠brido (int8 storage, float compute)
- Ideal para AVR sin FPU

**Estructura del C√≥digo**:
```c
#include <avr/pgmspace.h>

// Pesos cuantificados en Flash
const int8_t W1[N][M] PROGMEM = {...};
// Escalas en Flash
const float sW1[N] PROGMEM = {...};
// Biases originales en Flash
const float B1[N] PROGMEM = {...};

void predict(float *row, float *out) {
    // Descuantificaci√≥n on-the-fly
    float w = (float)pgm_read_byte(&W1[i][j]) * pgm_read_float(&sW1[i]);
    // C√°lculo en float
    sum += w * input[j];
}
```

**Ventajas**:
- Ahorra SRAM (datos en Flash)
- Port√°til (sin dependencias)
- Compatible con Arduino Uno/Nano

---

## Tabla Comparativa

| Caracter√≠stica | Per-Layer Symmetric | CMSIS-NN Fixed-Point | Modo H√≠brido AVR |
|----------------|---------------------|----------------------|-------------------|
| **Precisi√≥n de Pesos** | int8 | int8 | int8 |
| **Precisi√≥n de Biases** | int32 | int32 | float32 |
| **Precisi√≥n de Activaciones** | float32 (ref) | int8 | float32 |
| **Escala** | Por capa | Por capa | Por capa |
| **Zero-Point** | 0 (symmetric) | 0 (symmetric) | 0 (symmetric) |
| **Almacenamiento Pesos** | RAM/Flash | RAM (alineado) | PROGMEM (Flash) |
| **Velocidad Inferencia** | Media | Muy Alta | Baja-Media |
| **Consumo Energ√≠a** | Medio | Bajo | Medio |
| **Memoria Requerida** | ~75% menos | ~75% menos | ~75% menos (pesos) |
| **Compatibilidad** | Universal | ARM Cortex-M | AVR 8-bit |
| **Dependencias** | Ninguna | CMSIS-NN | Ninguna |
| **FPU Requerido** | Opcional | No | S√≠ (emulaci√≥n OK) |
| **SIMD Optimizado** | No | S√≠ | No |
| **Recomendado para** | General | Cortex-M4/M7 | Arduino Uno/Nano |
| **Overhead de C√≥digo** | Bajo | Medio | Bajo |
| **Complejidad** | Baja | Media | Baja |

### M√©tricas de Rendimiento T√≠picas

| M√©trica | Float32 Original | Per-Layer (int8) | CMSIS-NN | H√≠brido AVR |
|---------|------------------|------------------|----------|-------------|
| **Tama√±o Modelo** | 100% | ~25% | ~25% | ~25% (pesos) |
| **Velocidad Inferencia** | 1x | 2-3x | 5-10x | 1.5-2x |
| **P√©rdida Accuracy** | 0% | 0.5-2% | 0.5-2% | 0.5-2% |
| **RAM Usada** | Alta | Media | Media | Baja |
| **Flash Usada** | Baja | Media | Media | Alta |

---

## Limitaciones Actuales

### Limitaciones T√©cnicas

1. **Solo Redes Neuronales**
   - ‚úÖ `MiniNeuralNetwork` soporta cuantificaci√≥n completa
   - ‚ùå Otros modelos (DecisionTree, RandomForest, etc.) no soportan cuantificaci√≥n

2. **Cuantificaci√≥n Post-Entrenamiento**
   - ‚ùå No hay Quantization-Aware Training (QAT)
   - ‚ùå La cuantificaci√≥n ocurre despu√©s del entrenamiento
   - ‚ö†Ô∏è Puede haber p√©rdida de precisi√≥n en modelos sensibles

3. **Per-Layer (No Per-Channel)**
   - ‚ùå Una escala por capa, no por canal
   - ‚ö†Ô∏è Menor precisi√≥n que per-channel para redes grandes
   - ‚úÖ Suficiente para redes peque√±as-medianas

4. **Activaciones en Float (Modo Nativo)**
   - ‚ö†Ô∏è En `to_arduino_code()`, las activaciones se calculan en float
   - ‚úÖ Solo CMSIS-NN usa activaciones int8 puras
   - ‚ö†Ô∏è Requiere FPU o emulaci√≥n para modo h√≠brido

5. **Limitado a MLP (2 Capas)**
   - ‚ùå Solo soporta arquitecturas de 2 capas ocultas
   - ‚ùå No soporta redes profundas (3+ capas)
   - ‚úÖ Suficiente para la mayor√≠a de casos embebidos

6. **Calibraci√≥n Requiere Dataset**
   - ‚ö†Ô∏è `calibrate()` necesita dataset completo
   - ‚ö†Ô∏è No hay calibraci√≥n con dataset reducido
   - ‚úÖ Se ejecuta autom√°ticamente despu√©s de `fit()`

### Limitaciones de Hardware

1. **CMSIS-NN**: Solo ARM Cortex-M
2. **Modo H√≠brido**: Requiere FPU o emulaci√≥n de float
3. **PROGMEM**: Solo disponible en AVR

---

## Recomendaciones para Proyectos Embebidos

### ¬øQu√© M√©todo Usar?

#### üèÜ **Recomendaci√≥n Principal: CMSIS-NN (si est√° disponible)**

**Para**: ARM Cortex-M4, M7, M33, M55
- ‚úÖ M√°xima velocidad y eficiencia
- ‚úÖ Inferencia completamente en int8
- ‚úÖ Bajo consumo de energ√≠a
- ‚úÖ Kernels optimizados con SIMD

**Ejemplo de Uso**:
```python
# Entrenar y exportar
model = MiniNeuralNetwork(n_inputs=2, n_hidden=4, n_outputs=1)
model.fit(dataset)
# Cuantificaci√≥n autom√°tica en export_to_c()
code = ml_manager.export_to_c("my_model")
```

#### ü•à **Segunda Opci√≥n: Modo H√≠brido AVR**

**Para**: Arduino Uno, Nano, y otros AVR 8-bit
- ‚úÖ Ahorra SRAM (datos en Flash)
- ‚úÖ Port√°til (sin dependencias)
- ‚úÖ Compatible con hardware limitado

**Ejemplo de Uso**:
```python
# Similar al anterior, pero export_to_c() usar√° to_arduino_code()
# si CMSISAdapter no est√° disponible
code = ml_manager.export_to_c("my_model")
```

#### ü•â **Tercera Opci√≥n: Per-Layer Nativo**

**Para**: Proyectos que requieren m√°xima portabilidad
- ‚úÖ Sin dependencias externas
- ‚úÖ Funciona en cualquier plataforma
- ‚ö†Ô∏è Menor velocidad que CMSIS-NN

### Gu√≠a de Selecci√≥n por Hardware

| Hardware | MCU | RAM | Flash | FPU | Recomendaci√≥n |
|----------|-----|-----|-------|-----|---------------|
| **STM32F4** | Cortex-M4 | 192KB | 1MB | S√≠ | CMSIS-NN |
| **STM32F7** | Cortex-M7 | 512KB | 2MB | S√≠ | CMSIS-NN |
| **Arduino Uno** | AVR | 2KB | 32KB | No | Modo H√≠brido |
| **ESP32** | Xtensa | 520KB | 4MB | S√≠ | CMSIS-NN (si portado) |
| **Raspberry Pi Pico** | Cortex-M0+ | 264KB | 2MB | No | Modo H√≠brido |

### Mejores Pr√°cticas

1. **Siempre Calibrar con Dataset Representativo**
   ```python
   # Usar el mismo dataset de entrenamiento
   model.fit(training_dataset)  # Calibra autom√°ticamente
   ```

2. **Validar Precisi√≥n Post-Cuantificaci√≥n**
   ```python
   # Comparar accuracy antes y despu√©s
   accuracy_before = evaluate(model, test_set)
   model.quantize()
   accuracy_after = evaluate_quantized(model, test_set)
   assert accuracy_after >= accuracy_before - 0.02  # Tolerancia 2%
   ```

3. **Usar Escalado de Inputs**
   ```python
   # El scaler ayuda a mantener rangos consistentes
   ml_manager.train_pipeline(
       model_name="model",
       dataset=data,
       model_type="neural_network",
       scaling="minmax"  # Recomendado
   )
   ```

4. **Optimizar Arquitectura para Cuantificaci√≥n**
   - Usar activaciones ReLU (m√°s amigables a cuantificaci√≥n)
   - Evitar capas muy profundas
   - Limitar rango de pesos durante entrenamiento

---

## Ejemplos de Uso

### Ejemplo 1: Entrenamiento y Exportaci√≥n Completa

```python
from miniml import ml_manager

# Dataset de ejemplo (XOR)
dataset = [
    [0.0, 0.0, 0],
    [0.0, 1.0, 1],
    [1.0, 0.0, 1],
    [1.0, 1.0, 0]
]

# Entrenar con escalado
result = ml_manager.train_pipeline(
    model_name="xor_nn",
    dataset=dataset,
    model_type="neural_network",
    params={
        "n_inputs": 2,
        "n_hidden": 4,
        "n_outputs": 1,
        "epochs": 2000,
        "learning_rate": 0.1
    },
    scaling="minmax"
)

# Exportar a C (cuantificaci√≥n autom√°tica)
c_code = ml_manager.export_to_c("xor_nn")

# Guardar c√≥digo
with open("xor_model.h", "w") as f:
    f.write(c_code)
```

### Ejemplo 2: Cuantificaci√≥n Manual

```python
from miniml.ml_runtime import MiniNeuralNetwork

# Crear y entrenar modelo
model = MiniNeuralNetwork(n_inputs=2, n_hidden=4, n_outputs=1)
model.fit(dataset)

# Calibraci√≥n (autom√°tica despu√©s de fit, pero se puede hacer manual)
model.calibrate(dataset)

# Cuantificaci√≥n expl√≠cita
model.quantize(per_channel=True)

# Verificar cuantificaci√≥n
print(f"Quantized: {model.quantized}")
print(f"Act scales: {model.act_scales}")
print(f"W1 shape: {len(model.q_W1)}x{len(model.q_W1[0])}")
```

### Ejemplo 3: Uso de CMSISAdapter Directamente

```python
from miniml.ml_runtime import MiniNeuralNetwork
from adapters.cmsis_nn.adapter import CMSISAdapter

# Entrenar modelo
model = MiniNeuralNetwork(n_inputs=2, n_hidden=4, n_outputs=1)
model.fit(dataset)
model.quantize()

# Generar c√≥digo CMSIS-NN
adapter = CMSISAdapter(model)
adapter.generate_c("model_cmsis.h")
```

### Ejemplo 4: Guardar y Cargar Modelo Cuantificado

```python
# Guardar modelo (incluye act_scales y pesos cuantificados)
ml_manager.save_model("xor_nn", "xor_nn.json")

# Cargar modelo (restaura act_scales)
ml_manager.load_model("xor_nn", "xor_nn.json")

# Re-cuantificar si es necesario
model = ml_manager.get_model("xor_nn")
if not model.quantized:
    model.quantize()

# Exportar
c_code = ml_manager.export_to_c("xor_nn")
```

---

## Conclusi√≥n

MiniML ofrece un sistema de cuantificaci√≥n robusto y flexible para redes neuronales, con tres modos principales adaptados a diferentes plataformas:

1. **CMSIS-NN**: M√°xima velocidad para ARM Cortex-M
2. **Modo H√≠brido AVR**: Ideal para Arduino 8-bit
3. **Per-Layer Nativo**: Port√°til y universal

La cuantificaci√≥n reduce el tama√±o del modelo en ~75% y acelera la inferencia 2-10x, con una p√©rdida t√≠pica de precisi√≥n < 2%, haci√©ndola ideal para aplicaciones embebidas de IA.

---

**√öltima actualizaci√≥n**: 2024
**Versi√≥n MiniML**: 1.0.0
